{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gramelon2003/a/blob/master/BTKN03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9JKyW4ohWET"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from scipy.optimize import minimize, rosen, rosen_der\n",
        "\n",
        "# Sklearn processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sklearn regression algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Sklearn regression model evaluation function\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "def readData(filePath: str, filename: str):\n",
        "    data = np.loadtxt(os.path.join(filePath, filename), delimiter = ',')\n",
        "    X = data[:,:-1]\n",
        "    y = data[:, -1]\n",
        "    m = X.shape[0]\n",
        "    n = X.shape[1]\n",
        "    X = np.reshape(X, (m,n))\n",
        "    y = np.reshape(y, (m,1))\n",
        "    #Them cot x0 = 1 vao X\n",
        "    x0 = np.ones((m,1))\n",
        "    X = np.column_stack([x0, X])\n",
        "    return X, y\n",
        "\n",
        "def featureVectorScaling(data):\n",
        "    avg = np.mean(data)\n",
        "    sln = data.max()\n",
        "    snn = data.min()\n",
        "    data_scl = (data - avg)/(sln - snn)\n",
        "    print(data_scl[1])\n",
        "    return data_scl\n",
        "\n",
        "def normalizeData(X):\n",
        "    X_scl = X[:, 0]\n",
        "    for i in range(1, X.shape[1]):\n",
        "        scl = featureVectorScaling(X[:, i])\n",
        "        X_scl = np.column_stack([X_scl, scl])\n",
        "    return X_scl\n",
        "\n",
        "#ham hw(X)\n",
        "def sigmoid(X, w):\n",
        "    result = 1/(1 + np.exp(-np.dot(X, w)))\n",
        "    return result\n",
        "\n",
        "def loss(X, y, w):\n",
        "    m = y.shape[0]\n",
        "    result = (-1/m)*np.sum(np.dot(y.T, np.log(sigmoid(X, w))) + np.dot((1 - y).T, np.log(1 - sigmoid(X, w))))\n",
        "    return result\n",
        "\n",
        "def gradient(X, y, w):\n",
        "    m = X.shape[0]\n",
        "    result = (1/m)*np.dot(X.T, sigmoid(X, w) - y)\n",
        "    return result\n",
        "\n",
        "def gradientDescent(X, y, w, alpha, n_iters):\n",
        "    w_optimal = w.copy()\n",
        "    J_history = []\n",
        "    for i in range(n_iters):\n",
        "        w_optimal = w_optimal - alpha*gradient(X, y, w_optimal)\n",
        "        J_history.append(loss(X, y, w_optimal))\n",
        "    return w_optimal, J_history\n",
        "\n",
        "#Hàm dự đoán nếu y_pred >=0.5 làm tròn thành 1, ngược lại là 0\n",
        "def predict(y_pred):\n",
        "    return np.rint(y_pred)\n",
        "\n",
        "def acc_score(y, y_hat):\n",
        "    m = y.shape[0]\n",
        "    result = (1/m)*np.sum(y == y_hat)\n",
        "    return  result\n",
        "\n",
        "def main():\n",
        "    X, y = readData('D:/data/hocmay', 'ex2data1.txt')\n",
        "    X = normalizeData(X)\n",
        "    n = X.shape[1]\n",
        "    w = np.zeros((n, 1))\n",
        "    alpha = 0.01\n",
        "    n_iters = 2000\n",
        "    #Chia train - test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        test_size=0.30,\n",
        "                                                        random_state=15)\n",
        "    w_opt, J_hist = gradientDescent(X_train, y_train, w, alpha, n_iters)\n",
        "    print(\"Ket qua huan luyen mo hinh la: \")\n",
        "    print('\\t\\tTrong so w toi uu la: ', w_opt)\n",
        "    print('\\t\\tGia tri Loss toi uu: ', J_hist[-1])\n",
        "    print('Ket qua du doan cua mo hinh')\n",
        "    y_hat = predict(sigmoid(X_test, w_opt))\n",
        "    print('\\t\\tMột số kết quả dự đoán: ', y_hat[:5,:])\n",
        "    print('\\t\\tChỉ số Accuracy: ', acc_score(y_test, y_hat))\n",
        "    print('\\t\\tSử dụng sklearn, Acc: ', accuracy_score(y_test.flatten(), y_hat.flatten()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "else:\n",
        "    print ('Executed when imported')"
      ]
    }
  ]
}